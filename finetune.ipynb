{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemma-3 270m Fine-tuning with Unsloth-MLX\n",
        "\n",
        "This notebook demonstrates how to fine-tune the Gemma-3 270m model locally on Apple Silicon using `unsloth-mlx`. This tool wraps Apple's MLX framework for efficient Lora fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: ./gemma3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from unsloth_mlx import FastLanguageModel\n",
        "import mlx.core as mx\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "model_path = os.getenv(\"MLX_MODEL_PATH\", \"./gemma3\")\n",
        "print(f\"Loading model from: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Tokenizer\n",
        "We load the Gemma-3 270m model. Ensure the files are present in the directory specified above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_path,\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True, # Use 4-bit quantization to save memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a74c0757",
      "metadata": {},
      "source": [
        "> *Note:*\n",
        "\n",
        "> `load_in_4bit = True, # Use 4-bit quantization to save memory`\n",
        "\n",
        "> It perform QLoRA as Uses 4-bit quantization, allowing you to run models that simply won't fit with standard LoRA.\n",
        " For a small model like Gemma-3 270m, both will be extremely fast. However, using QLoRA (4-bit) is \"better\" for your Mac because:\n",
        " It leaves more RAM free for other system tasks.\n",
        " It allows you to use longer context lengths (more tokens) without crashing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Add LoRA Adapters\n",
        "LoRA (Low-Rank Adaptation) allows us to fine-tune only a small fraction of the model's parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA configuration set: rank=16, alpha=16, modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], dropout=0\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e5affba",
      "metadata": {},
      "source": [
        "## 3. Prepare Dataset\n",
        "We load the locally generated dataset. The path is managed via the `DATASET_PATH` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2346eb27",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the locally generated dataset from .env path\n",
        "dataset_file = os.getenv(\"DATASET_PATH\", \"dataset.jsonl\")\n",
        "dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must match the prompt format expected by Gemma-3\n",
        "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "\n",
        "print(f\"Loaded {len(dataset)} examples from {dataset_file}\")\n",
        "print(\"Sample training text:\")\n",
        "print(dataset[0][\"text\"]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training\n",
        "Unsloth-MLX provides a simple trainer wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth_mlx import SFTTrainer\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     model = model,\n",
        "#     tokenizer = tokenizer,\n",
        "#     train_dataset = dataset,\n",
        "#     dataset_text_field = \"text\",\n",
        "#     max_seq_length = 2048,\n",
        "#     args = {\n",
        "#         \"learning_rate\": 2e-4,\n",
        "#         \"num_train_epochs\": 1,\n",
        "#         \"logging_steps\": 1,\n",
        "#     },\n",
        "# )\n",
        "\n",
        "print(\"Trainer setup complete. Uncomment the code above to start training.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
