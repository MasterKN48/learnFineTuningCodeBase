{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8784ca33",
      "metadata": {},
      "source": [
        "# Gemma-3 270m Fine-tuning with Unsloth-MLX\n",
        "\n",
        "This notebook demonstrates how to fine-tune the Gemma-3 270m model locally on Apple Silicon using `unsloth_mlx`. This library optimizes Apple's MLX framework for significantly faster Lora fine-tuning with less memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ee26296a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: ./gemma3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from unsloth_mlx import FastLanguageModel\n",
        "import mlx.core as mx\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "model_path = os.getenv(\"MLX_MODEL_PATH\", \"./gemma3\")\n",
        "print(f\"Loading model from: {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d9c159",
      "metadata": {},
      "source": [
        "## 1. Load Model and Tokenizer\n",
        "\n",
        "We use `FastLanguageModel.from_pretrained` to load both the weights and the tokenizer. \n",
        "\n",
        "### Arguments Explained:\n",
        "- **model_name**: The local path or HuggingFace ID of the model.\n",
        "- **max_seq_length**: The maximum context window (number of tokens) the model will handle. Setting this higher uses more VRAM.\n",
        "- **load_in_4bit**: Enables **4-bit NormalFloat (NF4)** quantization. This shrinks the model size by ~4x, allowing it to fit into much smaller RAM/VRAM while maintaining high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60309f7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_path,\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True, # Use 4-bit quantization to save memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6e6037",
      "metadata": {},
      "source": [
        "## 2. Add LoRA Adapters\n",
        "\n",
        "Instead of updating billions of parameters, **LoRA (Low-Rank Adaptation)** adds small trainable \"adapters\" to specific layers. We only train these small matrices, making training much faster and lightweight.\n",
        "\n",
        "### Arguments Explained:\n",
        "- **r (Rank)**: Controls the size of the adapter matrices. `16` is a standard value that balances performance and efficiency.\n",
        "- **target_modules**: Specifies which layers of the model to attach adapters to (e.g., Query, Key, Value projections in Attention).\n",
        "- **lora_alpha**: A scaling factor for the adapters. Usually set to the same as `r` or $2 \\times r$.\n",
        "- **lora_dropout**: Dropout probability for the adapters. Set to `0` for best performance in most cases.\n",
        "- **bias**: Whether to train biases. `none` is recommended for standard LoRA.\n",
        "- **use_gradient_checkpointing**: Saves VRAM by recalculating parts of the network during the backward pass instead of storing all activations.\n",
        "- **random_state**: Ensures reproducibility of the initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a3835bb8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA configuration set: rank=16, alpha=16, modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], dropout=0\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e322f80",
      "metadata": {},
      "source": [
        "## 3. Prepare Dataset\n",
        "\n",
        "We load our `dataset.jsonl` file and format it into strings the model can understand. The Gemma-3 model expects instructions to be clearly delineated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3084ce27",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the locally generated dataset from .env path\n",
        "dataset_file = os.getenv(\"DATASET_PATH\", \"dataset.jsonl\")\n",
        "dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        # The model is trained to generate the output given the instruction\n",
        "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "\n",
        "print(f\"Loaded {len(dataset)} examples from {dataset_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d13d8aed",
      "metadata": {},
      "source": [
        "## 4. Training\n",
        "\n",
        "The `SFTTrainer` manages the optimization loop. We use `SFTConfig` for better configuration and tracking.\n",
        "\n",
        "### Arguments Explained:\n",
        "- **train_dataset**: The processed data to learn from.\n",
        "- **dataset_text_field**: The key in our dataset containing the formatted strings.\n",
        "- **args**: High-level configuration via `SFTConfig`. This controls tracking, learning rates, and more.\n",
        "\n",
        "### SFTConfig Details:\n",
        "- **output_dir**: Where checkpoints and logs are stored.\n",
        "- **learning_rate**: How fast the model adjusts its weights. `2e-4` is standard for LoRA.\n",
        "- **num_train_epochs**: Pass count over the full dataset.\n",
        "- **logging_steps**: Frequency of progress updates.\n",
        "- **report_to**: Set to `\"tensorboard\"` to enable visual progress tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba854b40",
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth_mlx import SFTTrainer\n",
        "from trl import SFTConfig\n",
        "\n",
        "# 1. Define the Training Configuration\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./lora_finetuned\",\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=3,\n",
        "    max_seq_length=2048,\n",
        "    learning_rate=2e-4,\n",
        "    report_to=\"tensorboard\", # Logs progress for visualization\n",
        ")\n",
        "\n",
        "# 2. Initialize the Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    args = sft_config,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tb_guide",
      "metadata": {},
      "source": [
        "### Visualizing Progress with TensorBoard\n",
        "\n",
        "Since we enabled `report_to=\"tensorboard\"`, you can view the training loss and other metrics in real-time. \n",
        "Run the cell below to launch TensorBoard within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tb_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./lora_finetuned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ad98ea",
      "metadata": {},
      "source": [
        "## 5. Saving the Model\n",
        "\n",
        "We use `model.save_pretrained(\"lora_model\")` to save the results. \n",
        "\n",
        "### Saving Format:\n",
        "- **Format**: It saves the adapters in **`adapters.safetensors`** format along with config files (`adapter_config.json`). \n",
        "- **Efficiency**: It does **not** save the full multi-gigabyte base model. Instead, it only saves the few megabytes of LoRA weights we trained. \n",
        "- **Usage**: To use this later, you load the base Gemma-3 model and then \"attach\" these specific adapter files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ff9b0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "print(\"Model and tokenizer saved to './lora_model'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be1ba81",
      "metadata": {},
      "source": [
        "## 6. Testing (Inference)\n",
        "\n",
        "Finally, we run the model on a new prompt to see what it learned.\n",
        "\n",
        "### Why `for_inference`?\n",
        "- **FastLanguageModel.for_inference(model)**: Switches the model into an optimized generation mode (disables gradients, enables KV caching). This makes token generation significantly faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26556c20",
      "metadata": {},
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "prompt = \"### Instruction:\\nWrite the code for the file `index.html` in the codebase project.\\n\\n### Response:\"\n",
        "input_ids = mx.array(tokenizer.encode(prompt))\n",
        "\n",
        "output = model.generate(input_ids = input_ids, max_new_tokens = 128)\n",
        "print(tokenizer.decode(output))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
