{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemma-3 270m Fine-tuning with Unsloth-MLX\n",
        "\n",
        "This notebook demonstrates how to fine-tune the Gemma-3 270m model locally on Apple Silicon using `unsloth-mlx`. This tool wraps Apple's MLX framework for efficient Lora fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from unsloth_mlx import FastLanguageModel\n",
        "import mlx.core as mx\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "model_path = os.getenv(\"MLX_MODEL_PATH\", \"./gemma3\")\n",
        "print(f\"Loading model from: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Tokenizer\n",
        "We load the Gemma-3 270m model. Ensure the files are present in the directory specified above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_path,\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True, # Use 4-bit quantization to save memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Add LoRA Adapters\n",
        "LoRA (Low-Rank Adaptation) allows us to fine-tune only a small fraction of the model's parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Dataset\n",
        "Load your dataset here. For this template, we'll use a simple placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Example: Load a small dataset from HuggingFace or locally\n",
        "# dataset = load_dataset(\"json\", data_files=\"my_data.jsonl\", split=\"train\")\n",
        "\n",
        "print(\"Define your dataset processing logic here.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training\n",
        "Unsloth-MLX provides a simple trainer wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth_mlx import SFTTrainer\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     model = model,\n",
        "#     tokenizer = tokenizer,\n",
        "#     train_dataset = dataset,\n",
        "#     dataset_text_field = \"text\",\n",
        "#     max_seq_length = 2048,\n",
        "#     args = {\n",
        "#         \"learning_rate\": 2e-4,\n",
        "#         \"num_train_epochs\": 1,\n",
        "#         \"logging_steps\": 1,\n",
        "#     },\n",
        "# )\n",
        "\n",
        "print(\"Trainer setup complete. Uncomment the code above to start training.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
